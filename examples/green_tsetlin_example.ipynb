{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tsetlin Machine trains on IMDB\n",
    "This notebook shows how the green-tsetlin Tsetlin Machine trains on the IMDB sentiment dataset. With this tutorial, huggingface datasets library is used. The library holds the dataset that is on front preprocessed, removing symbols and white spaces.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobxtra/anaconda3/envs/sparse_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "imdb = datasets.load_dataset('imdb')\n",
    "x, y = imdb['train']['text'], imdb['train']['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn CountVectorizer\n",
    "\n",
    "With sklearn CountVectorizer, we can transform the data into bag-of-words.\n",
    "\n",
    "E.g the input text \"I love swimming in the ocean\" is transformed to : [0, 1, 1, 1, 0, 0] \\\n",
    "This vector is based on the vocabulary of the CountVectorizer, e.g [\"dogs\", \"love\", \"ocean\", \"swimming\", \"biking\", \"movie\"] \\\n",
    "We obtain the vocabulary by fitting the data. This gives us words / tokens that occur in the data. \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), binary=True, lowercase=True, max_features=5000)\n",
    "vectorizer.fit(x)\n",
    "\n",
    "x_bin = vectorizer.transform(x).toarray().astype(np.uint8)\n",
    "y = np.array(y).astype(np.uint32)\n",
    "\n",
    "shuffle_index = [i for i in range(len(x))]\n",
    "rng.shuffle(shuffle_index)\n",
    "\n",
    "x_bin = x_bin[shuffle_index]\n",
    "y = y[shuffle_index]\n",
    "\n",
    "x_bin = x_bin[:1000]\n",
    "y = y[:1000]\n",
    "\n",
    "train_x_bin, val_x_bin, train_y, val_y = train_test_split(x_bin, y, test_size=0.2, random_state=seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the green-tsetlin package using pip\n",
    "The green-tsetlin library offers cpu heavy and less cpu heavy implemenation of the library, offering systems with older cpus a plug-and-play version of the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install green-tsetlin\n",
    "#pip install green-tsetlin[cpu]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Green Tsetlin hpsearch\n",
    "\n",
    "With a number of different parameters to set in the TM, we can optimize by using the built in optuna optimizer. \\\n",
    "\n",
    "With this optimizer, we can set disired search spaces for each paramater. We Can also optimize for minimizing number of literals in each clause.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 14:27:13,898] A new study created in memory with name: IMDB hpsearch\n",
      "Processing trial 9 of 10, best score: [0.815]: 100%|██████████| 10/10 [01:08<00:00,  6.84s/it]\n"
     ]
    }
   ],
   "source": [
    "from green_tsetlin.hpsearch import HyperparameterSearch\n",
    "\n",
    "s_space = (2.0, 10.0)\n",
    "clause_space = (100, 500)\n",
    "threshold_space = (50, 1000)\n",
    "max_epoch_per_trial = 15\n",
    "literal_budget = (5, 10)\n",
    "minimize_literal_budget = False\n",
    "n_jobs = 5\n",
    "k_folds = 4\n",
    "\n",
    "imdb_hpsearch = HyperparameterSearch(s_space=s_space,\n",
    "    clause_space=clause_space,\n",
    "    threshold_space=threshold_space,\n",
    "    max_epoch_per_trial=max_epoch_per_trial,\n",
    "    literal_budget=literal_budget,\n",
    "    minimize_literal_budget=minimize_literal_budget,\n",
    "    seed=seed,\n",
    "    n_jobs=n_jobs,\n",
    "    k_folds=k_folds\n",
    "    )\n",
    "\n",
    "imdb_hpsearch.set_train_data(train_x_bin, train_y)\n",
    "imdb_hpsearch.set_test_data(val_x_bin, val_y)\n",
    "\n",
    "imdb_hpsearch.optimize(n_trials=10, study_name=\"IMDB hpsearch\", show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s': 9.607623603636291, 'n_clauses': 487, 'threshold': 497.09954100791236, 'literal_budget': 7}\n",
      "0.815\n"
     ]
    }
   ],
   "source": [
    "params = imdb_hpsearch.best_trials[0].params\n",
    "print(params)\n",
    "\n",
    "performance = imdb_hpsearch.best_trials[0].value \n",
    "print(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = imdb['test']['text'], imdb['test']['label']\n",
    "\n",
    "test_x_bin = vectorizer.transform(test_x).toarray().astype(np.uint8)\n",
    "test_y = np.array(test_y).astype(np.uint32)\n",
    "\n",
    "test_x_bin = test_x_bin[:500]\n",
    "test_y = test_y[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 15 of 15, train acc: 0.851, best test score: 0.788 (epoch: 11): 100%|██████████| 15/15 [00:02<00:00,  5.03it/s]\n",
      "Processing epoch 15 of 15, train acc: 0.885, best test score: 0.860 (epoch: 0): 100%|██████████| 15/15 [00:03<00:00,  4.83it/s]\n",
      "Processing epoch 15 of 15, train acc: 0.884, best test score: 0.860 (epoch: 2): 100%|██████████| 15/15 [00:03<00:00,  4.94it/s]\n",
      "Processing epoch 15 of 15, train acc: 0.900, best test score: 0.876 (epoch: 2): 100%|██████████| 15/15 [00:03<00:00,  4.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_test_score': 0.876, 'k_folds': 4}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from green_tsetlin.tsetlin_machine import TsetlinMachine\n",
    "from green_tsetlin.trainer import Trainer\n",
    "\n",
    "\n",
    "tm = TsetlinMachine(n_literals=test_x_bin.shape[1], \n",
    "                    \n",
    "                    n_clauses=params[\"n_clauses\"],\n",
    "                    s=params[\"s\"],\n",
    "                    threshold=int(params[\"threshold\"]),\n",
    "                    literal_budget=params[\"literal_budget\"],\n",
    "                    n_classes=2,\n",
    "                    )\n",
    "\n",
    "trainer = Trainer(tm=tm, n_jobs=5, n_epochs=15, seed=seed, progress_bar=True, k_folds=4)\n",
    "\n",
    "trainer.set_train_data(train_x_bin, train_y)\n",
    "trainer.set_test_data(val_x_bin, val_y)\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
