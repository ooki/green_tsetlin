{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tsetlin Machine trains on IMDB\n",
    "This notebook shows how the green-tsetlin Tsetlin Machine trains on the **IMDB sentiment dataset**. With this tutorial, huggingface datasets library is used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "imdb = datasets.load_dataset('imdb')\n",
    "x, y = imdb['train']['text'], imdb['train']['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn CountVectorizer\n",
    "\n",
    "With sklearn CountVectorizer, we can transform the data into bag-of-words.\n",
    "\n",
    "E.g the input text \"I love swimming in the ocean\" is transformed to : [0, 1, 1, 1, 0, 0] \\\n",
    "This vector is based on the vocabulary of the CountVectorizer, e.g [\"dogs\", \"love\", \"ocean\", \"swimming\", \"biking\", \"movie\"] \\\n",
    "We obtain the vocabulary by fitting the data. This gives us words / tokens that occur in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), binary=True, lowercase=True, max_features=5000)\n",
    "vectorizer.fit(x)\n",
    "\n",
    "x_bin = vectorizer.transform(x).toarray().astype(np.uint8)\n",
    "y = np.array(y).astype(np.uint32)\n",
    "\n",
    "shuffle_index = [i for i in range(len(x))]\n",
    "rng.shuffle(shuffle_index)\n",
    "\n",
    "x_bin = x_bin[shuffle_index]\n",
    "y = y[shuffle_index]\n",
    "\n",
    "x_bin = x_bin[:1000]\n",
    "y = y[:1000]\n",
    "\n",
    "train_x_bin, val_x_bin, train_y, val_y = train_test_split(x_bin, y, test_size=0.2, random_state=seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the green-tsetlin package using pip\n",
    "**The green-tsetlin library offers cpu heavy and less cpu heavy implemenation of the library, offering systems with older cpus a plug-and-play version of the library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install green-tsetlin\n",
    "#pip install green-tsetlin[cpu]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Green Tsetlin hyperparameter search\n",
    "\n",
    "With a number of different parameters to set in the TM, we can optimize by using the built in TM optuna optimizer, `green_tsetlin.hpsearch.HyperparameterSearch()`.\n",
    "\n",
    "HyperparameterSearch:\n",
    "\n",
    "- **search spaces**: Set a disired search space for each paramater. Either set the search space to a tuple, e.g (1, 4) will search between 1 and 4, or set it to a single value $\\\\$\n",
    "e.g 4 will only search on 4. `clause_space=(50, 250)` or `clause_space=125` \n",
    "\n",
    "- **literal budget**: Optimize for a minimum literal budget by setting `minimize_literal_budget=True`.\n",
    "\n",
    "- **Cross validation**: Set `k_folds=k` to an integer $k > 2$ to run cross validation k times on each trial\n",
    "\n",
    "HyperparameterSearch.optimize:\n",
    "\n",
    "- Run optimization over `n_trials`, store in database, e.g `\"sqlite:///my_database.db\"`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[See the Optuna documentation here:](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.create_study.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from green_tsetlin.hpsearch import HyperparameterSearch\n",
    "\n",
    "\n",
    "hpsearch = HyperparameterSearch(s_space=(2.0, 10.0),\n",
    "                                clause_space=(100, 500),\n",
    "                                threshold_space=(50, 1000),\n",
    "                                max_epoch_per_trial=15,\n",
    "                                literal_budget=(5, 10),\n",
    "                                k_folds=4,\n",
    "                                n_jobs=5,\n",
    "                                seed=42,\n",
    "                                minimize_literal_budget=False)\n",
    "\n",
    "hpsearch.set_train_data(train_x_bin, train_y)\n",
    "hpsearch.set_test_data(val_x_bin, val_y)\n",
    "\n",
    "hpsearch.optimize(n_trials=10, study_name=\"IMDB hpsearch\", show_progress_bar=True, storage=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "We get the results by calling `HyperparameterSearch().best_trials`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[See the Optuna documentation here:](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.best_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = hpsearch.best_trials[0].params\n",
    "performance = hpsearch.best_trials[0].values\n",
    "\n",
    "print(\"best paramaters: \", params)\n",
    "print(\"best score: \", performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = imdb['test']['text'], imdb['test']['label']\n",
    "\n",
    "test_x_bin = vectorizer.transform(test_x).toarray().astype(np.uint8)\n",
    "test_y = np.array(test_y).astype(np.uint32)\n",
    "\n",
    "test_x_bin = test_x_bin[:500]\n",
    "test_y = test_y[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from green_tsetlin.tsetlin_machine import TsetlinMachine\n",
    "from green_tsetlin.trainer import Trainer\n",
    "\n",
    "tm = TsetlinMachine(n_literals=test_x_bin.shape[1], \n",
    "                    \n",
    "                    n_clauses=params[\"n_clauses\"],\n",
    "                    s=params[\"s\"],\n",
    "                    threshold=int(params[\"threshold\"]),\n",
    "                    literal_budget=params[\"literal_budget\"],\n",
    "                    \n",
    "                    n_classes=2,\n",
    "                    )\n",
    "\n",
    "trainer = Trainer(tm=tm, n_jobs=5, n_epochs=15, seed=seed, progress_bar=True, k_folds=4)\n",
    "\n",
    "trainer.set_train_data(train_x_bin, train_y)\n",
    "trainer.set_test_data(val_x_bin, val_y)\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
